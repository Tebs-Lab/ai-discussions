# Exercise: Research the Basics of Neural Networks

In this exercise you'll be asked to do some research on your own in order to answer a few specific questions about neural networks. Afterwards we'll discuss these questions as a class.

## The Questions:

1. What is the "universal function approximation theorem" and why is it important in the context of neural networks?
2. What is a "loss function" and how is it used by a neural network?
    * How is this different from a "metric" such as accuracy or R^2?
3. What is "gradient descent" and how is it used in Neural Networks?
    * There are other "optimizers" besides gradient descent, can you find the names of some such algorithms? Can you describe any differences between this algorithm and gradient descent?
4. There is a very special relationship between these four following things. Attempt to define this relationship between:
    * The shape and values of the labels
    * The number of nodes in the final layer of the network
    * The activation function used in the final layer of the network
    * The loss function used.